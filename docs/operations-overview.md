---
layout: default
title: Operations Overview
description: How Geetanjali processes ethical consultations - from initial analysis to follow-up conversations.
---

# Operations Overview

How consultations flow through the system, from submission to response.

## Consultation Modes

Geetanjali offers two modes of interaction:

| Mode | Purpose | Pipeline |
|------|---------|----------|
| **Initial Consultation** | Full analysis of an ethical dilemma | RAG (retrieval + generation) |
| **Follow-up Conversation** | Clarification and refinement | Lightweight (context-only) |

Both modes process asynchronously, allowing the system to handle long-running LLM operations without blocking.

## Initial Consultation

When a user submits an ethical dilemma, the system performs a full RAG (retrieval-augmented generation) analysis:

```
User submits dilemma
        │
        ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ Content Filter  │────▶│ Vector Search   │────▶│ LLM Generation  │
│ (validation)    │     │ (find verses)   │     │ (structured)    │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                                                        │
                                                        ▼
                                               ┌─────────────────┐
                                               │ Structured      │
                                               │ Response        │
                                               │ - Summary       │
                                               │ - Options       │
                                               │ - Steps         │
                                               │ - Citations     │
                                               └─────────────────┘
```

**Output includes:**
- Executive summary
- Multiple options with tradeoffs
- Recommended action with steps
- Reflection prompts
- Verse citations with relevance scores

## Follow-up Conversations

After receiving guidance, users can ask follow-up questions for clarification or deeper exploration. Follow-ups use a lightweight pipeline that leverages existing context:

```
User asks follow-up
        │
        ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ Content Filter  │────▶│ Load Context    │────▶│ LLM Generation  │
│ (validation)    │     │ (prior output)  │     │ (conversational)│
└─────────────────┘     └─────────────────┘     └─────────────────┘
                                                        │
                                                        ▼
                                               ┌─────────────────┐
                                               │ Conversational  │
                                               │ Response        │
                                               │ (prose format)  │
                                               └─────────────────┘
```

**Differences from initial consultation:**
- No new verse retrieval (uses prior citations)
- Conversational prose output (not structured JSON)
- Rolling conversation history for context
- Faster response times

## Processing States

Consultations progress through defined states:

```
┌───────┐     ┌─────────┐     ┌────────────┐     ┌───────────┐
│ DRAFT │────▶│ PENDING │────▶│ PROCESSING │────▶│ COMPLETED │
└───────┘     └─────────┘     └────────────┘     └───────────┘
                                    │
                         ┌──────────┴──────────┐
                         ▼                     ▼
                  ┌──────────┐          ┌────────────────┐
                  │  FAILED  │          │POLICY_VIOLATION│
                  └──────────┘          └────────────────┘
```

| State | Meaning |
|-------|---------|
| DRAFT | Case created, not yet submitted |
| PENDING | Submitted, waiting for processing |
| PROCESSING | Analysis in progress |
| COMPLETED | Guidance ready |
| FAILED | Processing error (can retry) |
| POLICY_VIOLATION | Content policy triggered |

## Async Processing

Both consultation modes use asynchronous processing:

1. **Submission** — User submits request, receives immediate acknowledgment
2. **Queue** — Request is queued for background processing
3. **Processing** — Worker processes the request (LLM generation)
4. **Completion** — Status updates, results available

The frontend polls for status changes until processing completes. This architecture allows the system to handle concurrent requests efficiently and provide a responsive user experience even when LLM operations take time.

## Error Handling

| Scenario | Behavior |
|----------|----------|
| LLM timeout | Marked as FAILED, can retry |
| Invalid content | Returns educational response |
| Rate limit exceeded | Returns 429 with retry-after |
| Service unavailable | Fallback to secondary provider |

Failed consultations can be retried. The system maintains state to prevent duplicate processing.

## Rate Limits

To ensure fair usage and system stability:

| Operation | Limit |
|-----------|-------|
| Initial consultation | 10/hour |
| Follow-up questions | 30/hour |

Authenticated users share limits across sessions. Anonymous users are tracked by session.

## See Also

- [Architecture](architecture.md) — System components and data flow
- [Content Moderation](content-moderation.md) — How content filtering works
- [Setup Guide](setup.md) — Configuration options
